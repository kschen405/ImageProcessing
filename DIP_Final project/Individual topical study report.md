# Individual topical study report

## Introduction

In this report, I would like to explore the field of unknown-aware object detection with the assistance of semantic segmentation, which is one of the main topic in this course.

Detecting objects that are not seen from the previous training stage has become a trend in object detection. However, there are more and more findings suggest that the information semantic segmentation provides is extremely helpful for detecting those unknown objects. 

As a results, I will provide clear overviews for the following three papers, "Learning to Detect Every Thing in an Open World", "Residual Pattern Learning for Pixel-wise Out-of-Distribution Detection in Semantic Segmentation", "Open-Vocabulary Instance Segmentation via Robust Cross-Modal Pseudo-Labeling". All of them are recently proposed and accepted in top conferences.

## Methods

### Open-Vocabulary Instance Segmentation via Robust Cross-Modal Pseudo-Labeling (CVPR 2022)

https://arxiv.org/abs/2111.12698

![](https://i.imgur.com/BlqkMKd.png)

#### Open-Vocabulary

The concept of open-vocabulary in detection and segmentation is to leverage large-scale pretrained image-text models to provide additional information (textual modality) for the model. In this work, a pretrained Bert is introduced to convert object nouns appeared in the caption of the image to word embeddings. 

Moreover, the original fully connected layer for classification in Mask-RCNN will be replaced by an embedding layer that projects visual embeddings of region proposals to word embeddings. As such, we can calculate the cosine similarity between the word embedding of a region proposal and the corresponding word embedding provided by the pretrain Bert.

#### Open-Vocabulary Instance Segmentation 

The goal of open-vocabulary instance segmentation aims not only segmenting known classes with mask annotations but also unknown classes without annotations. With the aid of open-vocabulary as stated above, it is now possible to generate pseudo masks via word semantics and visual features. To move further, this paper proposed a mechanism to selectively distills mask knowledge from the teacher model to the student model by estimating the mask noise levels. In such fashion, the student model can surpass the upper-bound of the noisy teacher model, while leveraging the pseudo mask annotations generated by the teacher model.

#### Overall architecture
![](https://i.imgur.com/8hA50E1.png)

##### Teacher model
The goal of the teacher model is to use caption-image pairs to generate pseudo labels and learn a joint embedding space between word semantic and visual features.

As mentioned earilier, the teacher model is based on Mask-RCNN and replace the last fully connected layer with an embedding layer. The score of each region is based on the cosine similarity with the Bert word embedding for each class.

The main issue here is that the teahcer model along does not encode enough informaion when it comes to unknown objects. High-level information obtained from pretrained Bert cannot guarantee good ability in segmenation.

##### Cross-modal pseudo labeling
The purpose of cross-modal pseudo labeling is to distill information from word embeddings and aligned object regions into the student embedding head.

In this stage, the teacher model has not entered mask head, and the aligned object regions for each object noun in the caption is obtained. Student model is taught to predict the correct aligned object region for any given object noun.

![](https://i.imgur.com/p4VAYkG.png)


##### Pseudo-mask noises
To alleviate the noises in the pseudo mask generated by the teacher model, we need to estimate the noise level.

This paper made a assumption that each pixel in pseudo mask is corrupted by a Gaussian noise. Thus, we add randomly obtained Gaussian noises into every pixel and compute the binary cross entropy loss between the teacher mask and the corrupted student mask. Noted that although the Gaussian noises are randomly generated, the variance of it is a learnable parameter. To be more precise, the noisier the teacher mask is, the higher the variance is, the higher the noise level is.

![](https://i.imgur.com/OrbIXll.png)



##### Robust student model

To sum up, the higher the noise level we estimated from mask loss, the less reliable the aligned object region is. Therfore, the loss of cross-modal pseudo labeling should be weighted by the inverse of the variance.
![](https://i.imgur.com/ZXbiKiH.png)

![](https://i.imgur.com/AzViAuC.png)

### Residual Pattern Learning for Pixel-wise Out-of-Distribution Detection in Semantic Segmentation (preprint: 2022/11)

https://arxiv.org/abs/2211.14512

#### Out-of-Distribution Detection(OOD detection)
OOD detection is a subset of unknown-aware object detection. Unlike open-vocabulary object detection directly classifies unknown object to its corresponding class, OOD detection classifies all unknowns into the same class, unknown class. This is fairly reasonable because OOD detection does not rely on large pretrained models. 

A common way to detect OOD object is to use the energy formula.
![](https://i.imgur.com/xY9usD0.png)
where K indicates K known classes. fi(x) represents the logit of input x. T is a hyperparameter usually set to 1. The red regions below are the uncertain regions where energy score are large.
![](https://i.imgur.com/bL6W74n.png)

<!-- ![](https://i.imgur.com/1xOWbYp.png) -->

#### Overall architecture

The architecture includes a pretrained Mask-RCNN model that maintains good performance in known classes and a Residual Pattern Learning (RPL) head that boosts the performance in unknown classes. This setting allows the model to increase the accuracy in unknown classes, while preserving the accuracy in known classes.

![](https://i.imgur.com/dwFM6gT.png)

##### Datasets

To construct a dataset with mask annotations of OOD objects, this paper utlizes a method called Outlier Exposure (ICLR 2019). Imagine that you cut and paste the OOD mask annotations from other datasets in your original dataset without overlapping with the known object annotations. Here, we cut OOD objects from COCO dataset and paste them in CityScapes dataset.

![](https://i.imgur.com/7N6zcZL.png)

For validation, this paper adopts common OOD datasets called Fishyscape and Segment-Me-If-You-Can (SMIYC).

##### Residual pattern learning (RPL) 

RPL is trained to approximate the freezed segmentation model to preserve the performance of known classes.

However, RPL loss does not exclusively focus on known classes (In-distribution). 

![](https://i.imgur.com/aWHqYhV.png)

The positive energy loss (lout) specificly target the OOD objects and push their negative energy score up by maximizing them. (mi(w) is a indicator function, set to 1 when pixel w is OOD)

![](https://i.imgur.com/8mgfqEq.png)

##### Context-robust contrastive learning (CoroCL)

In addition to the original final layer of RPL, the paper adds another projection layer to generate embeddings of known and unknown objects. Then, apply instance-level contrastive learning to make each class objects more compact.


### Learning to Detect Every Thing in an Open World (ECCV 2022)

https://arxiv.org/abs/2112.01698

![](https://i.imgur.com/0m7LFXB.png)

#### Hidden object issue in object detection

Most of the existing object detection works ignore the fact that there are unannotated objects. These objects are explicitly learned as background in the training stage. The influence might be mild if only known classes appear in the testing stage. However, if we considered unknown objects during inference, the impact will become visible.

#### Overall architecture

To completely resolved the issue, an intuitive way is to create an customized background without any object and paste foreground objects in the customized background.

##### Background Erasing (BackErase)

![](https://i.imgur.com/BE4p5t1.png)

Background Erasing is actually very simple. Randomly sample a small region in the image and resize it to the size of the image. In such way, it is unlikely the sampled region contains foreground objects and even if it contains, after resizing, for example, 64 times, the object will be distroted and look dissimilar to an object.

After that, we paste the mask annotations in the customized background and create our own training images.


##### Decoupled Multi-Domain Training

![](https://i.imgur.com/fqZN5Bo.png)

In the previous step, we make sure that the background is not contaminated by hiddem objects. Nonetheless, new problem occurs. The images we trained on are in different a domain comparing to the original images. To elaborate, the frequency of the customized background is close to zero, so **the model might rely on the frequency information to detect objects**, rather than the actual visual information. 

Thus, the paper tackles the domain transfer issue by training a mask head on the original images. The function of mask head is to differntiate foreground and background, while the function of box regression is also differentiate foreground and background. As a result, the mask head trained on original images can help alliviate the domain transfer issue. Noticed that the mask head share the same background with the box head for customized images.

## Experiments

### Open-Vocabulary Instance Segmentation via Robust Cross-Modal Pseudo-Labeling (CVPR 2022)

![](https://i.imgur.com/Uu7uYRN.jpg)

The proposed method, XPM, obtain good mAP comparing to other caption-image and pseudo-labeling methods. However, **I don't think this is a fair comparison with other pseudo-labeling methods**, since they do not utlize captions in any of their images.

![](https://i.imgur.com/HRZwxOC.png)


### Residual Pattern Learning for Pixel-wise Out-of-Distribution Detection in Semantic Segmentation (preprint: 2022/11)

![](https://i.imgur.com/mqSAk1q.jpg)

As the picture shown above,the proposed method is able to detect small OOD objects, while maintaining low false positives. **It beats state-of-the-art methods with great margin.** The FPR is extremely low in most cases and AuPRC remains high.

![](https://i.imgur.com/ZCgMmmT.png)

### Hidden object issue in object detection

![](https://i.imgur.com/kPv0pD0.jpg)

As the image and the table show, the proposed method, LDET, significantly outperfroms Mask-RCNN on COCO dataset. It detects more objects than vanilla Mask-RCNN. **LDET is capable of detecting all foreground objects, including OOD objects.**
![](https://i.imgur.com/elr4uq9.png)

![](https://i.imgur.com/OHT2i0r.png)

## Discussions

Unknown-aware object detection is a large field to explore. The works in this field varies greatly. In this report, I introduce three representative papers trying to solve this task via the assistance of semantic segmentation. I think this line of works is meaningful and expect more future works in the field of unknown-aware object detection.
